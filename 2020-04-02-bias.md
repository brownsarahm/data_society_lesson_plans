title: Bias in AI Systems
description: DATA2080 Bias in AI class on April 2, 2020
author: Sarah Brown
authorurl: sarahmbrown.org
imgdir: biasimg
----
type:canvas
name: welcome
---

# Welcome to class 9

The next two classes will be about bias and fairness. First we'll talk about how AI systems become biased. We will look at this through a variety of lenses to get a broad view. We will examine both case studies of popular cases and more academic audits. This week we'll focus on understanding bias. We'll get specific about what it means for an AI system to be biased and how AI systems produce biased decisions.

----
type:canvas
name:readings
---
# Readings

[Propublica Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) this is a classic article in the fair machine learning community. In class, we'll work with their data and reproduce their results.

[Fair ML Book Classification Chapter](https://fairmlbook.org/pdf/classification.pdf) This textbook chapter lays out how nondiscrimination can be defined. In reading, keep track of the various definitions that appear and the running examples. Skip over the problem set and read the last bit of the chapter.

----
name: cover
---

# Welcome to 2080: Bias

## shared notes:

### drsmb.co/2080bias



----
type:animate
name: logistics
---

# In class Logistics

--
- questions
> zoom raise hand, that creates a queue, its' the most fair since I can't see all faces
> practice walk through finding, using nonverbal
--
- breakout questions
> either zoom request help (me or TA will join) or slack (all can see)
--
- polling and nonverbal feedback
> we will use these as deemed appropriates

----
name: admin
---
# Admin

_announcements_

----
name:goals
---

# Today's Learning Goals

_By the end of today's class_

1. Explain canonical case studies of biased AI systems
1. Identify sources of bias in AI systems
1. Measure bias in classification systems


----
type: animate
name: agenda
---

# Today's plan

--
1. Motivation and terms
--
1. Case study discussion & replication
--
_break_
--
1. Bias sources
--
1. Classification fairness metrics
--
1. Questions and challenges
> take the measures and look at how we can intervene and how those measures

----
type:animate
name: terms
---

## Bias

_ What do you mean by bias?_
--
- statistical bias: error
> not statistical bias
--
- social bias
> this is what we're looking for

----
type: animate
name: motivation
---

# Why Bias?

--
- social justice
> we want to treat people equitably, be inclusive, etc
--
- human rights
> equal treatment
--
- law
> anti discrimination is the law in the US in certain domains, in those

----
name: cases
---

# Case Studies

- ProPublica/ COMPAS
> journalists at propublica analyzed the COMPAS system and found bias, you'll replicate that and explore their data a little more
- Gender Shades
> an MIT Media Lab PHD student and MSR researcher audited 3 facial recognition APIs
- Word Embeddings
- Obermyer

----
name: compasintro
---

# COMPAS

Replication tutorial in breakout rooms
> in groups, you're going to follow a tutorial to replicate this analysis.

----
name: gendershades
---

# Gender Shades

[Gender Shades](http://gendershades.org/)
> see link

----
name: obermyer
---

# Obermyer

[paper on drive](https://drive.google.com/file/d/1n8TSZHY4M6ICpHc9rAip-vU8rTpHmrL6/view)
[paper on Science](https://science.sciencemag.org/content/366/6464/447)

----
type:img
name: obermyer-bias
---

# Disparity in Scores
> for the same score, black patients are sicker


----
type:img
name: obermyer-fairsim
---

# Simulated Fair Algorithm
> Simulated impact of a fair algorithm using actual illnesses instead of cost as a proxy, shows that more black patients would be refrerred for screen and automatically enrolled for extra care

----
name: amazonhiring
---

# Amazon Hiring

- built tool
- checked, found bad patterns
> field hockey
> women's sports

> any other examples

----
name: audits
---

# Types of auditing

> What were common patterns among those above?
- COMPAS: journalists gathered data related to where the tool was used including the system outputs, but not data like what when into the algorithm
- Gendershades: collected their own data they expected would be different from the training data, but similar enough to be a potential use case. They fed their data through the tools and provided a comparative audit of mulitple products to claim disparity in a type of technology
- Obermeyer: in collaboration with a hospital using the tool, they gathered real data and in collaboration with those that built the system were able to identify what the bigger problem was
- Amazon: detected the problem in quality control prior to deployment


----
name: break
---

# Break
## Get up and move!

----
name: bias
---

# Bias Sources, Measures, and Models

- Where does bias come from?
> Bias in Computer Systems
> Statistical Bias
> Cognitive biases
- How do we detect it?
- What does it look like?

----
name: biases
---

# Bias in Computer Systems

- preexisting
- technical
- emergent


----
name: statbias
---

# Statistical Biases

- selection bias
> where you choose to collect data
- self selection bias
> who chooses to participate
- ommitted variable bias
> eg simpson's paradox

> bananas described as yellow
----
name:cogbias
---

# Cognitive Biases
- Confirmation bias
- observation bias
...
[many, many](https://en.wikipedia.org/wiki/List_of_cognitive_biases)

[worksheet](https://docs.google.com/spreadsheets/d/1Q17kqk--bUlrKR8xlNcJvH3NtpNkwUNEFA9rmGNYXQ8/edit#gid=0)

> go on that page and discuss in groups how a couple different ones might impact data analysis

----
name: dsbias
---

# Data Science Bias

- What other types of bias might be possible?
> venn diagram for what is DS
- Why is it even more important here?


----
name: socialmetrics
---

# Detecting Discrimination

- Disparate Impact
- Outcome tests

----
name: standards
---

# Formal Criteria

 Independence, Separation, Sufficiency


----
name: independence
---

# Independence

> Independence has been explored through many equivalent terms or variants, referred to as demographic parity, statistical parity, group fairness, disparate impact

----
name: separation
---

# Separation
> Roughly speaking, the separation criterion allows correlation between the score and the sensitive attribute to the extent that it is justified by the target variable.

----
name: sufficiency
---

# Sufficiency
> think of sufficiency in terms of positive and negative predictive values, thereâ€™s a useful alternative. Indeed, sufficiency turns out to be closely related to an important notion called calibration, as we will discuss next.

----
name:exclusive
---

# all are mutually exclusive

[case study](http://research.google.com/bigpicture/attacking-discrimination-in-ml/)

----
name: models
---

# Modeling Bias

- recent results on label bias

----
name: questions
---

# Questions and challenges

----
name: assignment
---

# Bias Audit Checklist

Find an AI system/application you're interested in. Analyze it for potential sources of bias and plan how you might evaluate it.

----
type: postnotes
name: assignmentdetail
---

# Bias Audit Checklist

Summary: Find an AI system you're interested in. The system must have some potential for bias. Analyze its potential for bias and plan how you would evaluate the system to determine if it was biased or not.

As evidence of your analysis, submit a brief (up to 3 pages) report that answers (at least) the following questions.


1. Identify the system with link
1. Summarize what it does (input, output, etc)
1. What sources of bias might this system be susceptible to?
1. What questions would you ask about the data used for training?
1. What EDA would you do if you had access to the training data?
1. What type of audit would you recommend?
1. What data would be needed for an audit?
1. What type of fairness metric(s) would you recommend for this and why?

An example template is available [here](https://www.overleaf.com/read/bvqnwqyzvkhw), but any format that answers the questions is acceptable.
You may add any additional sections that you think are helpful.

You will be graded on:
- clear concise writing overall (5 points)
- sufficiently detailed, but concise description of the system (5 points)
- appropriate, thorough assessment of possible biases (10 points)
- appropriate plan for determining if the system is biased (5 points)
- identification and justification of fairness metric(s) for the problem (5 points)

Recent years' KDD Applied Data Science track might be a good place to choose a candidate system to evaluate. They are listed under a heading "Applied Data Science..." in each year's accepted papers.

- [2019](https://www.kdd.org/kdd2019/accepted-papers)
- [2018](https://www.kdd.org/kdd2018/accepted-papers)
- [2017](https://www.kdd.org/kdd2017/accepted-papers)
- [2016](https://www.kdd.org/kdd2016/program/accepted-papers)
<!--
target audience:
- team that built the system
- journalist pitch
- impacted person's lawyer -->
