title: Bias in AI Systems
description: DATA2080 Bias in AI class on April 2, 2020
author: Sarah Brown
authorurl: sarahmbrown.org
imgdir: biasimg
----
type: canvas
name: welcome
---

# Welcome to week 10

This week we will look at strategies for mitigating bias in machine learning and what their limitations might be.  We'll conclude with a discussion about what other techniques might help and how things like interpretability, explanation, or data provenance might supplement algorithmic solutions.

----
type:canvas
name: readings
---


Read these two papers:
[A comparative study of fairness-enhancing interventions in machine learning](https://dl.acm.org/doi/10.1145/3287560.3287589) be prepared to critique the methodology of the evaluation. What additional things would you have done to evaluate the systems? What new questions do their results inspire?

[Fairness and Abstraction in Sociotechnical Systems](https://dl.acm.org/doi/10.1145/3287560.3287598) be prepared to generate concrete actions for a data scientist based on their recommendations for a more STS approach.

Read this [blog post on IBM's implementation of FairML tools](https://www.ibm.com/blogs/research/2018/09/ai-fairness-360/).

Optional, additional reading:
[Roles for computing in social change](https://dl.acm.org/doi/abs/10.1145/3351095.3372871)


----
name: cover
---

# Welcome to Fair ML

### shared doc: drsmb.co/2080fair


----
name:learninggoals
---
# Goals

1. Identify types of fair ML interventions
1. Apply fair ml techniques and metrics
1. Explain tradeoffs and limitations


----
name: agenda
---

1. Fair ML Interventions
1. What are the limitations of these approaches?


----
name: interventions
---

# Fair ML Interventions

- algorithmic (narrow) interventions
> these follow directly from definitions we saw last week
> keep the same problem, data, etc, and modify the solution with a purely algorithmic intervention
- system-level (broad) interventions
> these accept that we can intervene beyond the algorithm and range from collecting more data to


----
name:algointerventions
---

# Fair ML Algorithmic Interventions


- preprocessing
- in processing (regularization)
- post processing

----
name: preprocessing
---

# Preprocessing based techniques
> apply one from IBM360
- reweighting
-

----
name: inprocessing
---

# In processing based techniques
> apply one from IBM360

- adversarial (Adversarial Debiasing)
- regularize by fairness (PrejudiceRemover)

----
name: postprocessing
---

# Postprocessing
> apply one from IBM360

----
name: comparison
---

# Comparative Study of Fair ML

- review method
- discuss results
> breakout groups


----
name: break
---

# Break


----
name: algolimitations
---

# General Limitations

>in groups, brainstorm limitiations to the above

- causal understanding in observational data
- definitions are mutually exclusive
- which definition to choose
- bias/accuracy

----
name:chenwhy
---

# Why is my classifier discriminatory?

[paper](https://papers.nips.cc/paper/7613-why-is-my-classifier-discriminatory.pdf)

Synopsis other design choices drive perceived bias:
- sample size
- features
- sampling distribution

main prediction: majority voting over multiple datasets
optimal prediction: bayes optimal
per dataset prediction: prediction on a single dataset

bias: loss incurred by the main prediction relative to the optimal prediction.
variance: average loss incurred by the predictions learned from different datasets relative to
the main prediction
noise: the remaining loss independent of the learning algorithm, often known as the Bayes error

recommendations:
1. get more samples
2. measure more variables

----
name: moreaccuracy
---

# Label Bias

if there's label bias, a fair classifier can be more accurate

[Tradeoff Revisited](https://papers.nips.cc/paper/9082-unlocking-fairness-a-trade-off-revisited.pdf)

[Constraints Improve Accuracy](https://arxiv.org/pdf/1912.01094.pdf)

[Identifying and Correcting Label bias](https://arxiv.org/pdf/1901.04966.pdf)

----
name: systeminterventions
---

# System level interventions

- error type breakdown, add more samples or features
> where have we seen this succeed already?
- better variable selection


----
name: traps
---

# Fairness and Abstraction in Sociotechnical Systems

1. Framing Trap
1. Portability
1. Formalism
1. Ripple Effect
1. Solutionism


----
name: stslens
---

# STS Lens

1. Adopting a "heterogeneous engineering" approach
> re: framing
1. Contextualizing user "scripts"
> re portability
1. Identifying "interpretive flexibility," "relevant social groups,"
and "closure"
> re formalishm
1. Avoiding "reinforcement politics" and "reactivity"
> re: ripple efect
1. Considering when to design
> re solutionism

----
name: stsaction
---

# Questions to Consider in Design

(1) is appropriate to the situation in the first place, which requires a nuanced understanding of the relevant social context
and its politics (Solutionism);
(2) affects the social context in a predictable way such that the
problem that the technology solves remains unchanged after
its introduction (Ripple Effect);
(3) can appropriately handle robust understandings of social
requirements such as fairness, including the need for procedurality, contextuality, and contestability (Formalism);
(4) has appropriately modeled the social and technical requirements of the actual context in which it will be deployed
(Portability); and
(5) is heterogeneously framed so as to include the data and
social actors relevant to the localized question of fairness
(Framing).

> What actions might these questions prompt in differnt application areas?
> What safeguards or tests would help us consider these?

[worksheet](https://docs.google.com/presentation/d/14mDilEDeGum6H_2nLa_GzJ_8KGPP45DgoAUJpz5Ze2g/edit#slide=id.g731c04c73b_0_0)

1. work on the slide number of your breakout room
1. instructions in speaker notes
1. bold one person to lead sharing

> breakout deeper brainstorm
----
name: roles
---

# Roles for computing

> breakouts: how does it relate to the traps

- diagnostic
- formalizer
- rebuttal
- synechdoche

----
name: changes
---

# How to intervene, is it enough?

> tying together multiple topics


----
name: assignmenta
---

#

----

name: assignmentb
---

# Fair ML tutorial
## Optional, For bonus points

Write a tutorial for one of the techniques in the IBMF360 package that shows not only what the code does, but helps the reader understand the technique, which to use when, or the limits of algorithmic interventions.

> submit pdf of notebook on canvas or the notebook as PR to the [example folder in the data2080 fork of IBM/AIF360](https://github.com/data2080/AIF360/tree/master/examples)
